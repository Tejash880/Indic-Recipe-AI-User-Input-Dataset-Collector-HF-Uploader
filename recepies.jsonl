import pandas as pd
import json
import logging
from pathlib import Path
from typing import Optional, Dict, Any, List
import sys
from datetime import datetime

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def clean_text_field(value: Any) -> str:
    """Clean and normalize text fields"""
    if pd.isna(value) or value is None:
        return ""
    return str(value).strip()

def process_ingredients(ingredients_str: str) -> List[str]:
    """Process ingredients string into a clean list"""
    if not ingredients_str:
        return []
    
    # Split by common delimiters and clean
    if '\n' in ingredients_str:
        ingredients = [ing.strip() for ing in ingredients_str.split('\n')]
    elif ',' in ingredients_str:
        ingredients = [ing.strip() for ing in ingredients_str.split(',')]
    else:
        ingredients = [ingredients_str.strip()]
    
    # Remove empty items and normalize
    return [ing for ing in ingredients if ing]

def process_instructions(instructions_str: str) -> List[str]:
    """Process instructions string into clean steps"""
    if not instructions_str:
        return []
    
    # Split by line breaks and clean
    steps = [step.strip() for step in instructions_str.split('\n') if step.strip()]
    
    # If no line breaks, try to split by step indicators
    if len(steps) == 1:
        step_indicators = ['. ', ') ', '- ', '‚Ä¢ ']
        for indicator in step_indicators:
            if indicator in instructions_str:
                steps = [step.strip() for step in instructions_str.split(indicator) if step.strip()]
                break
    
    return steps

def convert_row_to_dict(row: pd.Series, include_all_fields: bool = False) -> Dict[str, Any]:
    """Convert a pandas row to a dictionary with proper processing"""
    # Core required fields
    record = {
        "name": clean_text_field(row.get("name", "")),
        "ingredients": process_ingredients(clean_text_field(row.get("ingredients", ""))),
        "instructions": process_instructions(clean_text_field(row.get("instructions", ""))),
        "language": clean_text_field(row.get("language", ""))
    }
    
    # Add optional fields if they exist and include_all_fields is True
    if include_all_fields:
        optional_fields = {
            "region": clean_text_field(row.get("region", "")),
            "prep_time_minutes": int(row.get("prep_time_minutes", 0)) if pd.notna(row.get("prep_time_minutes")) else 0,
            "cook_time_minutes": int(row.get("cook_time_minutes", 0)) if pd.notna(row.get("cook_time_minutes")) else 0,
            "total_time_minutes": int(row.get("total_time_minutes", 0)) if pd.notna(row.get("total_time_minutes")) else 0,
            "servings": int(row.get("servings", 0)) if pd.notna(row.get("servings")) else 0,
            "difficulty": clean_text_field(row.get("difficulty", "")),
            "image_filename": clean_text_field(row.get("image_filename", "")),
            "date_added": clean_text_field(row.get("date_added", ""))
        }
        
        # Only add non-empty optional fields
        for key, value in optional_fields.items():
            if value or (isinstance(value, int) and value > 0):
                record[key] = value
    
    return record

def validate_record(record: Dict[str, Any]) -> bool:
    """Validate that a record has required fields and meaningful content"""
    required_fields = ["name", "ingredients", "instructions", "language"]
    
    for field in required_fields:
        if field not in record or not record[field]:
            return False
    
    # Check if ingredients and instructions have meaningful content
    if len(record["ingredients"]) == 0 or len(record["instructions"]) == 0:
        return False
    
    return True

def csv_to_jsonl(
    csv_path: str = "recipes.csv",
    jsonl_path: str = "recipes.jsonl",
    include_all_fields: bool = False,
    validate_records: bool = True,
    chunk_size: Optional[int] = None
) -> Dict[str, int]:
    """
    Convert CSV to JSONL format with optimizations
    
    Args:
        csv_path: Path to input CSV file
        jsonl_path: Path to output JSONL file
        include_all_fields: Whether to include all available fields
        validate_records: Whether to validate records before writing
        chunk_size: Process CSV in chunks (useful for large files)
    
    Returns:
        Dictionary with conversion statistics
    """
    
    csv_file = Path(csv_path)
    jsonl_file = Path(jsonl_path)
    
    # Validate input file
    if not csv_file.exists():
        raise FileNotFoundError(f"CSV file not found: {csv_path}")
    
    logger.info(f"Converting {csv_path} to {jsonl_path}")
    logger.info(f"Include all fields: {include_all_fields}")
    logger.info(f"Validate records: {validate_records}")
    
    stats = {
        "total_rows": 0,
        "processed_rows": 0,
        "skipped_rows": 0,
        "valid_records": 0,
        "invalid_records": 0
    }
    
    try:
        # Create backup of existing JSONL file
        if jsonl_file.exists():
            backup_path = jsonl_file.with_suffix(f".backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl")
            jsonl_file.rename(backup_path)
            logger.info(f"Backed up existing file to: {backup_path}")
        
        # Process CSV in chunks if specified, otherwise read all at once
        if chunk_size:
            csv_reader = pd.read_csv(csv_path, chunksize=chunk_size, encoding='utf-8')
            write_mode = 'w'
        else:
            csv_reader = [pd.read_csv(csv_path, encoding='utf-8')]
            write_mode = 'w'
        
        with open(jsonl_path, write_mode, encoding='utf-8') as f:
            for chunk_idx, df_chunk in enumerate(csv_reader):
                logger.info(f"Processing chunk {chunk_idx + 1}, rows: {len(df_chunk)}")
                
                stats["total_rows"] += len(df_chunk)
                
                for idx, row in df_chunk.iterrows():
                    try:
                        # Convert row to dictionary
                        record = convert_row_to_dict(row, include_all_fields)
                        
                        # Validate if requested
                        if validate_records:
                            if not validate_record(record):
                                stats["invalid_records"] += 1
                                logger.warning(f"Skipping invalid record at row {idx}: {record.get('name', 'Unknown')}")
                                continue
                            stats["valid_records"] += 1
                        else:
                            stats["valid_records"] += 1
                        
                        # Write to JSONL
                        json.dump(record, f, ensure_ascii=False, separators=(',', ':'))
                        f.write('\n')
                        
                        stats["processed_rows"] += 1
                        
                    except Exception as e:
                        stats["skipped_rows"] += 1
                        logger.error(f"Error processing row {idx}: {e}")
                        continue
                
                # For chunked processing, change mode to append after first chunk
                if chunk_size and chunk_idx == 0:
                    f.close()
                    f = open(jsonl_path, 'a', encoding='utf-8')
        
        # Log statistics
        logger.info("Conversion completed!")
        logger.info(f"Total rows in CSV: {stats['total_rows']}")
        logger.info(f"Successfully processed: {stats['processed_rows']}")
        logger.info(f"Valid records: {stats['valid_records']}")
        logger.info(f"Invalid records: {stats['invalid_records']}")
        logger.info(f"Skipped rows (errors): {stats['skipped_rows']}")
        logger.info(f"Output file size: {jsonl_file.stat().st_size / 1024:.2f} KB")
        
        return stats
        
    except Exception as e:
        logger.error(f"Conversion failed: {e}")
        raise

def main():
    """Main function with command line argument support"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Convert CSV to JSONL format")
    parser.add_argument("--csv", default="recipes.csv", help="Input CSV file path")
    parser.add_argument("--jsonl", default="recipes.jsonl", help="Output JSONL file path")
    parser.add_argument("--all-fields", action="store_true", help="Include all available fields")
    parser.add_argument("--no-validate", action="store_true", help="Skip record validation")
    parser.add_argument("--chunk-size", type=int, help="Process CSV in chunks (for large files)")
    
    args = parser.parse_args()
    
    try:
        stats = csv_to_jsonl(
            csv_path=args.csv,
            jsonl_path=args.jsonl,
            include_all_fields=args.all_fields,
            validate_records=not args.no_validate,
            chunk_size=args.chunk_size
        )
        
        print(f"\n‚úÖ Conversion completed successfully!")
        print(f"üìä Processed {stats['processed_rows']}/{stats['total_rows']} rows")
        print(f"‚úÖ Valid records: {stats['valid_records']}")
        if stats['invalid_records'] > 0:
            print(f"‚ö†Ô∏è Invalid records: {stats['invalid_records']}")
        if stats['skipped_rows'] > 0:
            print(f"‚ùå Skipped rows: {stats['skipped_rows']}")
            
    except Exception as e:
        print(f"‚ùå Conversion failed: {e}")
        sys.exit(1)

if __name__ == "__main__":
    # For direct execution, run with default parameters
    if len(sys.argv) == 1:
        # No command line args, run with defaults
        stats = csv_to_jsonl(include_all_fields=True, validate_records=True)
        print(f"\n‚úÖ Conversion completed! Processed {stats['processed_rows']} recipes.")
    else:
        # Run with command line arguments
        main()